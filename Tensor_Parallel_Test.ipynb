{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOU160VAP96/rPCCYWg+0Wm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clam-sdx/tensor_parallel/blob/main/Tensor_Parallel_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TASmfUs0OwZh"
      },
      "outputs": [],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Among data parallelism, pipeline parallelism and tensor parallel, which ones help you if your model parameters cannot fit on a single GPU?\n",
        "\n",
        "- what is `torch.distributed.barrier()` for?\n",
        "\n",
        "- For training LLMs about many Gigabytes of vRAM would you expect to save if you trained a 7B model in 16-bit vs. 32-bit?\n"
      ],
      "metadata": {
        "id": "bXwuk1HRPqVq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 2: Tensor Parallelism\n",
        "Combining Column-Wise and Row-wise Tensor Parallelism\n",
        "\n",
        "*Suggested Time: 10 minutes*\n",
        "\n",
        "#### Background\n",
        "\n",
        "Two practical styles a linear transformation (linear layer) can be split up to distribute the work across GPUs are the row-wise way and column-wise way of tensor parallelism.\n",
        "\n",
        "In column- wise parallel, W is split horizontally (dim=1), X is duplicated across GPUs, and the outputs of the separate matmuls is concatenated (all_gather) along that same dimension horizontally (dim=1)\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/clam-sdx/tensor_parallel/refs/heads/main/notebooks/pics/column_wise_parallel.png\" height=250 width=500>\n",
        "\n",
        "In row-wise parallel, X is split horizontally (dim=1) and W is split vertically (dim=0). The i-th shards of X and W go to the same GPU together. There is no concatenation step but rather a elementwise addition (all_reduce) step at the end.\n",
        "\n",
        "Unlike Column-wise linear you cannot apply the non-linearity before the synchronization (all-gather, all-reduce) steps, because the activation function is non-linear, `f(a) + f(b) != f(a+b)` but `f(concat(a, b)) = concat(f(a), f(b))`\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/clam-sdx/tensor_parallel/refs/heads/main/notebooks/pics/row_wise_parallel.png\" height=250 width=500>\n",
        "\n",
        "For better or worse, we are in a jupyter notebook without access to real GPUs. So lets agree to represent which GPU we are using in our implementation by using intermediate variable names with `_GPU{index}` in the suffix like this:"
      ],
      "metadata": {
        "id": "nOANOSaLPK3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch; torch.manual_seed(42)\n",
        "from torch.nn.functional import gelu # f()\n",
        "\n",
        "# Non-Distributed Reference\n",
        "\n",
        "X = torch.randn(2, 4, device=\"cpu\", dtype=torch.float32)\n",
        "W = torch.randn(4, 4, device=\"cpu\", dtype=torch.float32)\n",
        "\n",
        "y = gelu(X @ W)\n",
        "\n",
        "print(f\"Non-Distributed Reference \\n{y}\\n\")\n",
        "\n",
        "# Column-Wise Distributed Linear\n",
        "\n",
        "# column-wise-split\n",
        "W_0, W_1 = W.chunk(2, dim=1) # W is split horizontally (dim=1)\n",
        "\n",
        "# distributed-matmul\n",
        "y_GPU0 = X @ W_0\n",
        "y_GPU1 = X @ W_1\n",
        "\n",
        "# non-linear activation function gelu()\n",
        "y_GPU0 = gelu(y_GPU0)\n",
        "y_GPU1 = gelu(y_GPU1)\n",
        "\n",
        "# all-gather\n",
        "y_col = torch.cat([y_GPU0, y_GPU1], dim=1)\n",
        "\n",
        "print(f\"Distributed Column-wise Linear \\n{y_col}\\n\")\n",
        "\n",
        "try:\n",
        "    torch.testing.assert_close(y_col, y, rtol=1e-5, atol=1e-5)\n",
        "    col_match = True\n",
        "    print(\"Column linear match\\n\")\n",
        "except AssertionError:\n",
        "    col_match = False\n",
        "    print(\"Column linear mismatch\\n\")\n",
        "\n",
        "# Row-Wise Distributed Linear\n",
        "\n",
        "# row-wise-split\n",
        "X_0, X_1 = X.chunk(2, dim=1) # X is split horizontally (dim=1)\n",
        "W_0, W_1 = W.chunk(2, dim=0) # W is split vertically (dim=0)\n",
        "\n",
        "# distributed-matmul\n",
        "\n",
        "y_GPU0 = X_0 @ W_0\n",
        "y_GPU1 = X_1 @ W_1\n",
        "\n",
        "# all-reduce\n",
        "y_GPU0 = y_GPU0 + y_GPU1\n",
        "\n",
        "# gelu\n",
        "y_row = gelu(y_GPU0)\n",
        "\n",
        "print(f\"Distributed Row-wise Linear \\n{y_row}\\n\")\n",
        "\n",
        "try:\n",
        "    torch.testing.assert_close(y_row, y, rtol=1e-5, atol=1e-5)\n",
        "    row_match = True\n",
        "    print(\"Row linear match\\n\")\n",
        "except AssertionError:\n",
        "    row_match = False\n",
        "    print(\"Row linear mismatch\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qg-kz_GDPMar",
        "outputId": "6625f924-532c-4394-c10a-d8232ef00cd4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Non-Distributed Reference \n",
            "tensor([[0.5324, 0.2612, 0.0106, 0.7086],\n",
            "        [1.1399, 2.8075, 1.8650, 1.5034]])\n",
            "\n",
            "Distributed Column-wise Linear \n",
            "tensor([[0.5324, 0.2612, 0.0106, 0.7086],\n",
            "        [1.1399, 2.8075, 1.8650, 1.5034]])\n",
            "\n",
            "Column linear match\n",
            "\n",
            "Distributed Row-wise Linear \n",
            "tensor([[0.5324, 0.2612, 0.0106, 0.7086],\n",
            "        [1.1399, 2.8075, 1.8650, 1.5034]])\n",
            "\n",
            "Row linear match\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Goal\n",
        "\n",
        "We want to do a linear transformation on a 2x4 input matrix `X` by first matrix multiplying it (`@`) with 4x4 parameter matrix `W1`, then applying a non-linear activation function `gelu()` to `X @ W1`, then applying one more linear transformation by matrix multiplying with another 4x4 parameter matrix `W2` to get `y`. The sequence of operations we just described can be represent by this expression:\n",
        "\n",
        "$$ y = gelu(X @ W1) @ W2 $$\n",
        "\n",
        "The sequence of operations we just described can be be implemented this way in a non-distributed manner:"
      ],
      "metadata": {
        "id": "VEC-BDpQPRXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch; torch.manual_seed(42)\n",
        "from torch.nn.functional import gelu # f()\n",
        "\n",
        "X = torch.randn(2, 4, device=\"cpu\", dtype=torch.float32)\n",
        "W1 = torch.randn(4, 4, device=\"cpu\", dtype=torch.float32)\n",
        "W2 = torch.randn(4, 4, device=\"cpu\", dtype=torch.float32)\n",
        "\n",
        "y_nondist = gelu(X @ W1) @ W2\n",
        "\n",
        "print(y_nondist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGVr9dqSPWZl",
        "outputId": "106f34bf-4b5d-4e43-e4e4-4136b4007d66"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.5733,  0.9400, -0.9233,  0.3600],\n",
            "        [-5.5034,  2.8632, -2.8932, -2.0718]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question\n",
        "\n",
        "Which method of distributing and synchronizing the operations of `gelu(X @ W1) @ W2` on multiple GPUs will be fastest in terms of minimizing data tansfer between GPUs while keeping the final result `y_dist` the same as had we not used parallelism and kept the computation on one CPU or GPU `y_nondist` ?\n",
        "\n",
        "1. column-wise-split -> distributed-matmul -> all-gather -> gelu -> column-wise-split -> distributed-matmul -> all-gather\n",
        "\n",
        "2. row-wise-parallel-split -> distributed-matmul -> all-reduce -> gelu -> row-wise-parallel-split -> distributed-matmul -> all-reduce\n",
        "\n",
        "3. column-wise-split -> distributed-matmul -> gelu -> distributed-matmul -> all-reduce\n",
        "\n",
        "4. row-wise-split -> distributed-matmul -> gelu -> distributed-matmul -> all-gather\n",
        "\n",
        "\n",
        "#### Implementation\n",
        "\n",
        "Write your solution below assuming you have 2 GPUs `GPU0` and `GPU1` at your disposal. Use intermediate variable names with `_GPU{index}` in the suffix just like in the examples above to indicate what operations are occuring in parallel."
      ],
      "metadata": {
        "id": "mIcPDNn6PehI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Write your solution here ##\n",
        "\n",
        "# Check if your y_dist matches the y_nondist\n",
        "\n",
        "# print(f\"Distributed Column-Row Linear \\n{y_dist}\\n\")\n",
        "\n",
        "# try:\n",
        "#     torch.testing.assert_close(y_dist, y_nondist, rtol=1e-5, atol=1e-5)\n",
        "#     col_row_match = True\n",
        "#     print(\"Column Row linear match\\n\")\n",
        "# except AssertionError:\n",
        "#     col_row_match = False\n",
        "#     print(\"Column Row linear mismatch\\n\")"
      ],
      "metadata": {
        "id": "a7GWySgbPfAj"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}